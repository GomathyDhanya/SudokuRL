{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **REINFORCEMENT LEARNING BASED SOLUTION FOR SOLVING A SUDOKU**"
      ],
      "metadata": {
        "id": "JF9d3lqLw-tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**State Representation**\n",
        "\n",
        "Sudoku boards are encoded as 9×9 grids.\n",
        "\n",
        "Each cell is one-hot encoded across 10 channels (digits 0–9), allowing the network to capture both filled and empty cells.\n",
        "\n",
        "This transforms the puzzle into a structured tensor input suitable for convolutional or dense neural layers.\n",
        "\n",
        "**Action Space**\n",
        "\n",
        "Defined as 729 discrete actions = 81 cells × 9 possible digits.\n",
        "\n",
        "Each action represents placing a number (1–9) in a specific cell.\n",
        "\n",
        "This extremely large action space makes exploration and convergence challenging.\n",
        "\n",
        "**Reward Structure**\n",
        "\n",
        "Positive rewards for correct placements aligning with Sudoku rules (valid digits in rows, columns, boxes).\n",
        "\n",
        "Penalties for illegal moves, repeated actions, or stagnation.\n",
        "\n",
        "Additional shaping:\n",
        "\n",
        "Rewards for completing rows, columns, or boxes.\n",
        "\n",
        "Stagnation counters prevent looping behavior.\n",
        "\n",
        "The goal is to encourage incremental correctness instead of only rewarding complete solutions.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "Environment: A custom SudokuEnv built on Gym, generating puzzles from the Kaggle dataset with adjustable clue fraction.\n",
        "\n",
        "Agent:\n",
        "\n",
        "*   Deep Q-Network (DQN) with epsilon-greedy exploration.\n",
        "*   Prioritized Replay Buffer to sample more useful experiences.\n",
        "*   Periodic updates with batches from memory.\n",
        "\n",
        "Training regime:\n",
        "\n",
        "*   Curriculum-style — start with easier puzzles (clue_fraction ~0.9) and gradually make them harder.\n",
        "*   Training loop executes thousands of attempts with capped max steps.\n",
        "*   Uses real-time rendering to observe puzzle filling during training.\n",
        "\n",
        "\n",
        "**Successes**\n",
        "*   The agent learns local constraints (e.g., avoiding duplicate numbers in a row/column).\n",
        "*   The reward shaping provides enough signal to progress beyond random guessing.\n",
        "*   The curriculum approach ensures stability on simpler puzzles before tackling harder ones.\n",
        "*   Demonstrates proof-of-concept: RL can capture logical structures in Sudoku.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Failures / Limitations**\n",
        "\n",
        "\n",
        "\n",
        "*   Exploration problem: With 729 possible actions per step, the agent wastes many steps exploring invalid moves.\n",
        "*   Sparse ultimate reward: Full puzzle completion is rare; most training focuses only on partial correctness.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ISdqqfCWoed8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VoN8XprBJ1e"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gymnasium import spaces\n",
        "import kagglehub\n",
        "from collections import deque\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load dataset\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "Jr2OAg52By4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"bryanpark/sudoku\")\n",
        "!mv /root/.cache/kagglehub/datasets/bryanpark/sudoku/versions/3 .\n"
      ],
      "metadata": {
        "id": "C9iOSWWuGF_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load puzzles and solutions into tensors\n",
        "\n",
        "sudoku_puzzles, sudoku_solutions = [], []\n",
        "with open('/kaggle/input/sudoku/sudoku.csv', 'r') as f:\n",
        "    for line in f.readlines()[1:]:\n",
        "        puzzle, solution = line.strip().split(\",\")\n",
        "        sudoku_puzzles.append([int(i) for i in puzzle])\n",
        "        sudoku_solutions.append([int(i) for i in solution])\n",
        "\n",
        "sudoku_puzzles = torch.tensor(sudoku_puzzles, dtype=torch.float32).view(-1, 9, 9).to(device)\n",
        "sudoku_solutions = torch.tensor(sudoku_solutions, dtype=torch.float32).view(-1, 9, 9).to(device)\n"
      ],
      "metadata": {
        "id": "ufIRO2E0jGL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudoku_puzzles[0]"
      ],
      "metadata": {
        "id": "8PfKmpPKjPlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we get a random solution and remove 1-clue_fraction amount of numbers\n",
        "def get_random_puzzle(clue_fraction=0.8):\n",
        "    idx = np.random.randint(len(sudoku_puzzles))\n",
        "    solution = sudoku_solutions[idx].clone()\n",
        "    puzzle = solution.clone()\n",
        "    mask = torch.rand_like(puzzle) > clue_fraction\n",
        "    puzzle[mask] = 0\n",
        "    return puzzle, solution\n"
      ],
      "metadata": {
        "id": "Eg6k78YEjeC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sudoku Gym environment\n",
        "class SudokuEnv(gym.Env):\n",
        "    def __init__(self, clue_fraction=0.8):\n",
        "        super().__init__()\n",
        "        self.action_space = spaces.Discrete(81 * 9) #81 cells, 9 numbers\n",
        "        self.observation_space = spaces.Box(low=0, high=9, shape=(9, 9), dtype=np.float32)\n",
        "        self.grid = None\n",
        "        self.solution = None\n",
        "        self.original_grid = None\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 5000\n",
        "        self.clue_fraction = clue_fraction\n",
        "        self.rewarded_rows = set()\n",
        "        self.rewarded_cols = set()\n",
        "        self.rewarded_boxes = set()\n",
        "        self.rewarded_cells = set()\n",
        "        self.recent_moves = deque(maxlen=10)\n",
        "        self.stagnation_counter = 0\n",
        "        self.temp_epsilon_boost = False\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.grid, self.solution = get_random_puzzle(clue_fraction=self.clue_fraction)\n",
        "        self.original_grid = self.grid.clone()\n",
        "        self.current_step = 0\n",
        "        self.rewarded_rows.clear()\n",
        "        self.rewarded_cols.clear()\n",
        "        self.rewarded_boxes.clear()\n",
        "        self.rewarded_cells.clear()\n",
        "        self.recent_moves.clear()\n",
        "        self.stagnation_counter = 0\n",
        "        self.temp_epsilon_boost = False\n",
        "        return self.grid.clone(), {}\n",
        "\n",
        "    #checks if rows, columns, boxes are completed\n",
        "    def check_subgoal_completion(self):\n",
        "        row_ok = [len(torch.unique(self.grid[r][self.grid[r] != 0])) == len(self.grid[r][self.grid[r] != 0]) for r in range(9)]\n",
        "        col_ok = [len(torch.unique(self.grid[:, c][self.grid[:, c] != 0])) == len(self.grid[:, c][self.grid[:, c] != 0]) for c in range(9)]\n",
        "        box_ok = []\n",
        "        for r in range(0, 9, 3):\n",
        "            for c in range(0, 9, 3):\n",
        "                box = self.grid[r:r+3, c:c+3].flatten()\n",
        "                nonzero = box[box != 0]\n",
        "                box_ok.append(len(torch.unique(nonzero)) == len(nonzero))\n",
        "        return row_ok, col_ok, box_ok\n",
        "\n",
        "    #Returns the number of conflicts in a grid for a given num in row and col\n",
        "    @staticmethod\n",
        "    def count_conflicts_static(grid, row, col, num):\n",
        "        conflicts = 0\n",
        "        conflicts += torch.sum(grid[row, :] == num).item() - (1 if grid[row, col] == num else 0)\n",
        "        conflicts += torch.sum(grid[:, col] == num).item() - (1 if grid[row, col] == num else 0)\n",
        "        r0, c0 = (row//3)*3, (col//3)*3\n",
        "        box = grid[r0:r0+3, c0:c0+3]\n",
        "        conflicts += torch.sum(box == num).item() - (1 if grid[row, col] == num else 0)\n",
        "        return int(conflicts)\n",
        "\n",
        "    def count_conflicts(self, row, col, num):\n",
        "        return SudokuEnv.count_conflicts_static(self.grid, row, col, num)\n",
        "\n",
        "    #renders the puzzel, correct - green, wrong red ( the agent ofcourse does not know if it's correct or wrong )\n",
        "    def render(self):\n",
        "        print(\"\\n\" + \"-\" * 25)\n",
        "        for i in range(9):\n",
        "            row_str = \"\"\n",
        "            for j in range(9):\n",
        "                val = int(self.grid[i, j])\n",
        "                if self.original_grid[i, j] != 0:\n",
        "                    # Original clues = normal\n",
        "                    cell = f\"{val}\"\n",
        "                else:\n",
        "                    if val == 0:\n",
        "                        cell = \".\"\n",
        "                    elif self.solution is not None and val == int(self.solution[i, j]):\n",
        "\n",
        "                        cell = f\"\\033[92m{val}\\033[0m\"\n",
        "                    else:\n",
        "\n",
        "                        cell = f\"\\033[91m{val}\\033[0m\"\n",
        "\n",
        "                sep = \" | \" if (j + 1) % 3 == 0 and j != 8 else \" \"\n",
        "                row_str += cell + sep\n",
        "            print(row_str)\n",
        "            if (i + 1) % 3 == 0 and i != 8:\n",
        "                print(\"------+-------+------\")\n",
        "        print(\"-\" * 25 + \"\\n\")\n",
        "\n",
        "    #every step of an episode has the following reward structure\n",
        "    def step(self, action):\n",
        "        pos, num = divmod(action, 9)\n",
        "        num += 1\n",
        "        row, col = divmod(pos, 9)\n",
        "\n",
        "        reward = 0.0\n",
        "        done = False\n",
        "\n",
        "        empty_cells_before = torch.sum(self.grid == 0).item()\n",
        "        row_ok_before, col_ok_before, box_ok_before = self.check_subgoal_completion()\n",
        "        score_before = sum(row_ok_before) + sum(col_ok_before) + sum(box_ok_before)\n",
        "        prev_val = self.grid[row, col].item()\n",
        "\n",
        "        if self.original_grid[row, col] != 0:\n",
        "            reward -= 1.0   # stronger penalty for changing clues\n",
        "        else:\n",
        "            self.grid[row, col] = num\n",
        "            conflicts = self.count_conflicts(row, col, num)\n",
        "\n",
        "            #penalty for conflicts\n",
        "            #we decide to remove wrong nubers from the board and penalize instead because the wrong number\n",
        "            #end up polluting the board due to sparse rewards vs penalities\n",
        "            if conflicts > 0:\n",
        "                reward -= 0.2 * conflicts\n",
        "                self.grid[row, col] = 0 if prev_val == 0 else prev_val\n",
        "            else:\n",
        "            #we reward for filling in a cell for the first time to encourage exploration\n",
        "                if prev_val == 0 and (row, col) not in self.rewarded_cells:\n",
        "                    filled_ratio = 1 - (empty_cells_before / 81)\n",
        "                    reward += 0.5 + 1.0 * filled_ratio\n",
        "                    self.rewarded_cells.add((row, col))\n",
        "                reward += 0.1\n",
        "\n",
        "\n",
        "        empty_cells_after = torch.sum(self.grid == 0).item()\n",
        "        row_ok_after, col_ok_after, box_ok_after = self.check_subgoal_completion()\n",
        "        score_after = sum(row_ok_after) + sum(col_ok_after) + sum(box_ok_after)\n",
        "\n",
        "        if score_after > score_before:\n",
        "            reward += 0.5 * (score_after - score_before)\n",
        "\n",
        "        #rewards first time for subgoals\n",
        "        for r in range(9):\n",
        "            if row_ok_after[r] and r not in self.rewarded_rows:\n",
        "                reward += 10.0\n",
        "                self.rewarded_rows.add(r)\n",
        "        for c in range(9):\n",
        "            if col_ok_after[c] and c not in self.rewarded_cols:\n",
        "                reward += 10.0\n",
        "                self.rewarded_cols.add(c)\n",
        "        for b in range(9):\n",
        "            if box_ok_after[b] and b not in self.rewarded_boxes:\n",
        "                reward += 10.0\n",
        "                self.rewarded_boxes.add(b)\n",
        "\n",
        "        #rewards if the cells are all filled or complete\n",
        "        if empty_cells_after == 0:\n",
        "            reward += 40.0\n",
        "            if all(row_ok_after) and all(col_ok_after) and all(box_ok_after):\n",
        "                reward += 200.0\n",
        "                done = True\n",
        "\n",
        "        if empty_cells_after == empty_cells_before:\n",
        "            self.stagnation_counter += 1\n",
        "        else:\n",
        "            self.stagnation_counter = 0\n",
        "\n",
        "        #if the state stays stagnant penalize\n",
        "        if self.stagnation_counter > 50:\n",
        "            reward -= 2.0\n",
        "\n",
        "        #stop doing the same move over and over again\n",
        "        if (row, col, num) in self.recent_moves:\n",
        "            reward -= 0.1\n",
        "        self.recent_moves.append((row, col, num))\n",
        "\n",
        "        #to encourage faster completion\n",
        "        reward -= 0.01\n",
        "\n",
        "        self.current_step += 1\n",
        "        return self.grid.clone(), reward, done, False, {}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fB7_sYEIkDh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dueling Q-Network\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(10, 32, 3, 1, 1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * 9 * 9, 512)\n",
        "\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self.adv_stream = nn.Sequential(\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 10, 9, 9)\n",
        "        x = self.conv_layers(x)\n",
        "        x = F.relu(self.fc(x))\n",
        "        value = self.value_stream(x)\n",
        "        adv = self.adv_stream(x)\n",
        "        return value + adv - adv.mean(dim=1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "SgCnP1dLmxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prioritized replay buffer to retain those instances with max TDerror\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, max_size=10000, alpha=0.6):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "        self.priorities = deque(maxlen=max_size)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def add(self, exp, td_error=1.0):\n",
        "        self.buffer.append(exp)\n",
        "        self.priorities.append(abs(td_error) + 1e-5)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        probs = np.array(self.priorities) ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        return zip(*samples), indices\n",
        "\n",
        "    def update_priorities(self, indices, td_errors):\n",
        "        for i, td in zip(indices, td_errors):\n",
        "            self.priorities[i] = abs(td.item()) + 1e-5\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "ApkwkhICnADH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The game playing agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, tau=0.01, device=\"cuda\"):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        #Double DQN\n",
        "        self.q_network = DuelingQNetwork(state_size, action_size).to(device)\n",
        "        self.target_network = DuelingQNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
        "        self.loss_fn = nn.SmoothL1Loss()\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay_steps = 100_000\n",
        "        self.epsilon_decay_rate = (self.epsilon - self.epsilon_min) / self.epsilon_decay_steps\n",
        "        self.steps_done = 0\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "        self.update_target_network(hard=True)\n",
        "\n",
        "    def update_target_network(self, hard=False):\n",
        "        if hard:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        else:\n",
        "            for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "                target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    def act(self, state, grid=None, original_grid=None, inference=False, temp_epsilon_boost=False):\n",
        "        self.steps_done += 1\n",
        "        if not inference:\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay_rate)\n",
        "\n",
        "        effective_epsilon = self.epsilon\n",
        "        if temp_epsilon_boost:\n",
        "            effective_epsilon = max(0.3, effective_epsilon)\n",
        "\n",
        "        if not inference and np.random.rand() <= effective_epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_network(state)\n",
        "\n",
        "            # Mask invalid actions during inference\n",
        "            if inference and grid is not None and original_grid is not None:\n",
        "                for pos in range(81):\n",
        "                    row, col = divmod(pos, 9)\n",
        "                    if original_grid[row, col] != 0:  # fixed clue\n",
        "                        q_values[0, pos*9:(pos+1)*9] = -1e9\n",
        "                    else:\n",
        "                        for num in range(1, 10):\n",
        "                            if SudokuEnv.count_conflicts_static(grid, row, col, num) > 0:\n",
        "                                q_values[0, pos*9+num-1] = -1e9\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def train(self, batch, indices=None, replay_buffer=None):\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "        states = torch.stack(states).to(self.device)\n",
        "        next_states = torch.stack(next_states).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Double DQN\n",
        "        next_actions = self.q_network(next_states).argmax(1)\n",
        "        next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.loss_fn(q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.update_target_network(hard=False)\n",
        "\n",
        "        if replay_buffer and indices is not None:\n",
        "            td_errors = (q_values - target_q_values).detach()\n",
        "            replay_buffer.update_priorities(indices, td_errors)"
      ],
      "metadata": {
        "id": "ikb4rGlwlPwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def one_hot_encode(grid, device=\"cuda\"):\n",
        "    one_hot = torch.zeros(10, 9, 9, device=device)\n",
        "    for i in range(9):\n",
        "        for j in range(9):\n",
        "            value = int(grid[i, j])\n",
        "            one_hot[value, i, j] = 1.0\n",
        "    return one_hot.unsqueeze(0)"
      ],
      "metadata": {
        "id": "wMYp9rp_nTFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trainging loop for the agent\n",
        "def train_and_resolve_multiple_times(agent, replay_buffer, env, num_attempts=10, max_steps=None, render_frequency=10):\n",
        "    for attempt in range(num_attempts):\n",
        "        env.clue_fraction = max(0.3, env.clue_fraction - 0.0005 * attempt)\n",
        "        grid, _ = env.reset()\n",
        "        original_grid = grid.clone()\n",
        "        state = one_hot_encode(grid, device=agent.device)\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state, temp_epsilon_boost=env.temp_epsilon_boost)\n",
        "            next_grid, reward, done, _, _ = env.step(action)\n",
        "            next_state = one_hot_encode(next_grid, device=agent.device)\n",
        "\n",
        "            if replay_buffer.size() > 64:\n",
        "                batch, indices = replay_buffer.sample(64)\n",
        "                agent.train(batch, indices, replay_buffer)\n",
        "\n",
        "            replay_buffer.add((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if max_steps and step_count >= max_steps:\n",
        "                break\n",
        "\n",
        "            if step_count % render_frequency == 0:\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Attempt {attempt + 1}, Step {step_count}, Reward: {total_reward:.2f}, Clue Fraction: {env.clue_fraction:.2f}\")\n",
        "                env.render()\n",
        "\n",
        "        print(f\"Attempt {attempt+1} finished | Steps: {step_count} | Total Reward: {total_reward:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Tn3pZxZWoBX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SudokuEnv(clue_fraction=0.90)\n",
        "replay_buffer = PrioritizedReplayBuffer(max_size=20000)\n",
        "agent = DQNAgent(state_size=10*9*9, action_size=81*9, device=device)\n",
        "\n",
        "train_and_resolve_multiple_times(\n",
        "    agent,\n",
        "    replay_buffer,\n",
        "    env,\n",
        "    num_attempts=500,\n",
        "    max_steps=2500,\n",
        "    render_frequency=50\n",
        ")"
      ],
      "metadata": {
        "id": "2D02aiqPoRH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}